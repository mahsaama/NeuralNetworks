{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Comparing optimizers on the Beale Function (3.0 points)\n",
    "The [Beale function](https://www.sfu.ca/~ssurjano/beale.html) is a non-convex function that is often used as a test problem for optimization algorithms. It has a global minimum of 0 at the point (3, 0.5). The function is defined bx[1]:\n",
    "$$f(x, y) =  \\left(1.5 - x + xy\\right)^{2} + \\left(2.25 - x + xy^{2}\\right)^{2} + \\left(2.625 - x + xy^{3}\\right)^{2}$$\n",
    "We will be using the Beale function to visualize some of Pytorch's built-in optimization algorithms. Implement the Beale function in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pt\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib import ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beale(x):\n",
    "    return (1.5 - x[0] + x[0]*x[1])** 2 + (2.25 - x[0] + x[0]* x[1]**2)**2 + (2.625 - x[0] + x[0]* x[1]**3)**2\n",
    "plot_space = ((-4.5, 4.5), (-4.5, 4.5))\n",
    "MIN = (3, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Beale function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pt.figure()\n",
    "ax = fig.add_subplot(projection=\"3d\", azim=-128, elev=43)\n",
    "s = 0.05\n",
    "\n",
    "X = np.arange(plot_space[0][0], plot_space[0][1] + s, s)\n",
    "Y = np.arange(plot_space[1][0], plot_space[1][1] + s, s)\n",
    "xmesh, ymesh = np.meshgrid(X, Y)\n",
    "fmesh = beale(np.array([xmesh, ymesh]))\n",
    "ax.plot_surface(\n",
    "    xmesh,\n",
    "    ymesh,\n",
    "    fmesh,\n",
    "    rstride=1,\n",
    "    cstride=1,\n",
    "    norm=LogNorm(),\n",
    "    linewidth=0,\n",
    "    edgecolor=\"none\",\n",
    "    cmap=\"jet\",\n",
    ")\n",
    "\n",
    "# Set the axis limits so that they are the same as in the figure above.\n",
    "ax.set_xlim(plot_space[0])\n",
    "ax.set_ylim(plot_space[1])\n",
    "\n",
    "pt.xlabel(\"x1\")\n",
    "pt.ylabel(\"x2\")\n",
    "pt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmesh, ymesh = np.mgrid[plot_space[0][0]:plot_space[0][1]:50j, plot_space[1][0]:plot_space[1][1]:50j]\n",
    "fmesh = beale(np.array([xmesh, ymesh]))\n",
    "pt.axis(\"auto\")\n",
    "\n",
    "pt.contourf(xmesh, ymesh, fmesh, locator=ticker.LogLocator(base=3, numticks=20), cmap=\"jet\")\n",
    "pt.annotate(\"Min\", MIN)\n",
    "pt.xlabel(\"x1\")\n",
    "pt.ylabel(\"x2\")\n",
    "pt.title('Contour plot - colors in log scale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement gradient function for Beale function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_trajectory(trajectory: list[np.ndarray]):\n",
    "    \"\"\"\n",
    "    The function gets a list of points the optimization has visited, and plots them over the function's contour plot\n",
    "    \"\"\"\n",
    "    print(f\"Num of iterations: {len(trajectory)}\")\n",
    "    xmesh, ymesh = np.mgrid[\n",
    "        np.min(trajectory) - 1 : np.max(trajectory) + 1 : 50j,\n",
    "        np.min(trajectory) - 1 : np.max(trajectory) + 1 : 50j,\n",
    "    ]\n",
    "    fmesh = beale(np.array([xmesh, ymesh]))\n",
    "    pt.axis(\"auto\")\n",
    "    pt.contourf(xmesh, ymesh, fmesh, locator=ticker.LogLocator(base=3, numticks=20), cmap=\"jet\")\n",
    "    array = np.array(trajectory)\n",
    "\n",
    "    pt.plot(array.T[0], array.T[1], \"r--\")\n",
    "    pt.plot(array[-1][0], array[-1, 1], \"y*\", )\n",
    "    pt.annotate(\n",
    "        \"Initial\",\n",
    "        (array[0][0], array[0][1]),\n",
    "        xytext=(0, 5),\n",
    "        textcoords='offset fontsize',\n",
    "        arrowprops=dict(facecolor=\"white\", shrink=0.05),\n",
    "    )\n",
    "    pt.annotate(\n",
    "        \"Final\",\n",
    "        (array[len(array) - 1][0], array[len(array) - 1][1]),\n",
    "        xytext=(0, -5),\n",
    "        textcoords='offset fontsize',\n",
    "        arrowprops=dict(facecolor=\"white\", shrink=0.05),\n",
    "    )\n",
    "    pt.colorbar()\n",
    "    print(\n",
    "        \"Min of the function is at:\"\n",
    "        + str((array[len(array) - 1][0], array[len(array) - 1][1]))\n",
    "    )\n",
    "    print(f\"Function value at this point is {beale(array[-1])}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the following starting points for all algorithms. Feel free to experiment how the starting point influences the outcome of optimization!\n",
    "\n",
    "**All algorithms should (almost) converge to the optimum for the very easy starting point**\n",
    "\n",
    "You can read on all the following optimization algorithms on the lecture slides or the [GoodFellow book, Optimization Chapter](https://www.deeplearningbook.org/contents/optimization.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_point = np.array([2,0]) #very easy\n",
    "# starting_point = np.array([1.1, 1.5]) #easy\n",
    "# starting_point = np.array([-1, 3]) #hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD with Momentum\n",
    "**Implement SGD with momentum method that returns the trajectory of the gradient descent. By trajectory we mean the ordered list of 2D points visited by your algorithm. Please include the starting point, too.**\n",
    "\n",
    "**Experiment with different values of momentum and learning rate explain your findings with the help of visualizations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_with_momentum(starting_point, lr, momentum=0.9, epsilon=0.001) -> list[np.ndarray]:\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "trajectory = sgd_with_momentum(starting_point, lr)\n",
    "visualize_trajectory(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSProp\n",
    "**Implement RMSprop method that returns the trajectory of the gradient descent.**\n",
    "**Experiment with different values of beta for RMSProp and explain your findings with the help of visualizations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsprop(starting_point, lr=0.001, beta=0.9, epsilon=.001, max_iters=1e5):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "trajectory = rmsprop(starting_point, lr)\n",
    "visualize_trajectory(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaGrad\n",
    "**Implement AdaGrad method that returns the trajectory of the gradient descent.**\n",
    "**Experiment with different values of learning rate and explain your findings with the help of visualizations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adagrad(starting_point, lr=0.01, epsilon=0.001, max_iters=1e5):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "trajectory = adagrad(starting_point, lr)\n",
    "visualize_trajectory(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement Adam method that returns the trajectory of the gradient descent.**\n",
    "**Experiment with different values of beta1 and beta2 and weight_decay for Adam and explain your findings with the help of visualizations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(starting_point, lr=0.001, beta1=0.9, beta2=0.999, epsilon=0.001, max_iters=1e5):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "trajectory = adam(starting_point, lr)\n",
    "visualize_trajectory(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdamW\n",
    "Traditional Adam optimizer intertwines weight decay with its learning rate, which can lead to suboptimal regularization. AdamW addresses this by applying weight decay directly to the weights, independent of the optimizer's adaptive learning rate mechanism. This approach aligns more closely with how weight decay is implemented in classical optimizers like Stochastic Gradient Descent (SGD). You can read more about AdamW here https://openreview.net/pdf?id=Bkg6RiCqY7.\n",
    "\n",
    "**Implement AdamW method that returns the trajectory of the gradient descent.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adamw(\n",
    "    starting_point,\n",
    "    lr=0.001,\n",
    "    beta1=0.9,\n",
    "    beta2=0.999,\n",
    "    epsilon=0.001,\n",
    "    max_iters=1e5,\n",
    "    weight_decay=0.01,\n",
    "):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "trajectory = adamw(starting_point, lr)\n",
    "visualize_trajectory(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is the difference between Adam and AdamW negligible ? If so, explain why.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing Newton Raphson method for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1) Implement hessian function for the Beale function.**\n",
    "\n",
    "**(2) Implement newton_raphson method that returns the trajectory. Consider keeping the number of iterations low for this one. It easily diverges.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hessian(x):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def newton_raphson(starting_point, epsilon=0.001, max_iters=40) -> list[np.ndarray]:\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory = newton_raphson(starting_point)\n",
    "visualize_trajectory(trajectory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
