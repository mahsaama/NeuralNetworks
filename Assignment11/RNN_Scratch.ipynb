{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vx1J6vQ3elko",
        "origin_pos": 0
      },
      "source": [
        "# Implementation of RNN from Scratch (6 points)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "wBCJXF8pelkp",
        "origin_pos": 3,
        "outputId": "33528fe1-b9b3-4058-81ad-e2bc47204d31",
        "tab": [
          "tensorflow"
        ]
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import re\n",
        "import math\n",
        "import collections\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building Corpus and Vocabulary (1 point)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following code cells, we provide a simple method to build a corpus and vocabulary based on the data we have. Before reviewing them and answering the following questions:\n",
        "1. (0.5 Points) Differentiate between corpus and vocabulary. \n",
        "2. (0.5 Points) The `<unk>` token is widely used in NLP tasks; explain the intuition behind using this token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3617"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def read_data(path='./data/timemachine.txt'):\n",
        "    with open(path, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
        "lines = read_data()\n",
        "len(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize(lines, token='word'):\n",
        "    \"\"\"Split text lines into word or character tokens.\"\"\"\n",
        "    if token == 'word':\n",
        "        return [line.split() for line in lines]\n",
        "    elif token == 'char':\n",
        "        return [list(line) for line in lines]\n",
        "    else:\n",
        "        print('ERROR: unknown token type: ' + token)\n",
        "\n",
        "tokens = tokenize(lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    \"\"\"Vocabulary for text.\"\"\"\n",
        "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
        "        if tokens is None:\n",
        "            tokens = []\n",
        "        if reserved_tokens is None:\n",
        "            reserved_tokens = []\n",
        "        # Sort according to frequencies\n",
        "        counter = count_corpus(tokens)\n",
        "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
        "                                   reverse=True)\n",
        "        # The index for the unknown token is 0\n",
        "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
        "        self.token_to_idx = {token: idx\n",
        "                             for idx, token in enumerate(self.idx_to_token)}\n",
        "        for token, freq in self._token_freqs:\n",
        "            if freq < min_freq:\n",
        "                break\n",
        "            if token not in self.token_to_idx:\n",
        "                self.idx_to_token.append(token)\n",
        "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)\n",
        "\n",
        "    def __getitem__(self, tokens):\n",
        "        if not isinstance(tokens, (list, tuple)):\n",
        "            return self.token_to_idx.get(tokens, self.unk)\n",
        "        return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "    def to_tokens(self, indices):\n",
        "        if not isinstance(indices, (list, tuple)):\n",
        "            return self.idx_to_token[indices]\n",
        "        return [self.idx_to_token[index] for index in indices]\n",
        "\n",
        "    @property\n",
        "    def unk(self):  # Index for the unknown token\n",
        "        return 0\n",
        "\n",
        "    @property\n",
        "    def token_freqs(self):  # Index for the unknown token\n",
        "        return self._token_freqs\n",
        "\n",
        "def count_corpus(tokens):\n",
        "    \"\"\"Count token frequencies.\"\"\"\n",
        "    # Here `tokens` is a 1D list or 2D list\n",
        "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
        "        # Flatten a list of token lists into a list of tokens\n",
        "        tokens = [token for line in tokens for token in line]\n",
        "    return collections.Counter(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('<unk>', 0), ('the', 1), ('and', 2), ('of', 3), ('i', 4), ('a', 5), ('to', 6), ('in', 7), ('was', 8), ('that', 9)]\n"
          ]
        }
      ],
      "source": [
        "vocab = Vocab(tokens)\n",
        "print(list(vocab.token_to_idx.items())[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(35850, 4937)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def load_corpus_time_machine(max_tokens=-1):\n",
        "    \"\"\"Return token indices and the vocabulary of the time machine dataset.\"\"\"\n",
        "    lines = read_data()\n",
        "    tokens = tokenize(lines, 'word')\n",
        "    vocab = Vocab(tokens)\n",
        "    # Since each text line in the time machine dataset is not necessarily a\n",
        "    # sentence or a paragraph, flatten all the text lines into a single list\n",
        "    corpus = [token for line in tokens for token in line]\n",
        "    if max_tokens > 0:\n",
        "        corpus = corpus[:max_tokens]\n",
        "    return corpus, vocab\n",
        "\n",
        "corpus, vocab = load_corpus_time_machine()\n",
        "len(corpus), len(vocab)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loader(corpus, num_steps=4, batch_size=32):\n",
        "    corpus = corpus[random.randint(0, num_steps-1):]\n",
        "    num_subseqs = (len(corpus) - 1) // num_steps\n",
        "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
        "    random.shuffle(initial_indices)\n",
        "\n",
        "    def data(pos):\n",
        "        return vocab[corpus[pos: pos+num_steps]]\n",
        "\n",
        "    num_batches = num_subseqs // batch_size\n",
        "    for i in range(0, batch_size * num_batches, batch_size):\n",
        "\n",
        "        initial_indices_per_batch = initial_indices[i: i + batch_size]\n",
        "        X = [data(j) for j in initial_indices_per_batch]\n",
        "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
        "        yield torch.tensor(X), torch.tensor(Y)\n",
        "\n",
        "class SeqLoader:\n",
        "    def __init__(self, corpus=corpus, vocab=vocab, num_steps=4, batch_size=32):\n",
        "        self.corpus = corpus\n",
        "        self.vocab = vocab\n",
        "        self.num_steps = num_steps\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        return loader(self.corpus, self.num_steps, self.batch_size)\n",
        "\n",
        "train_iter = SeqLoader(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh8YTUyNelkq",
        "origin_pos": 6
      },
      "source": [
        "### One-Hot Encoding\n",
        "\n",
        "\n",
        "\n",
        "Note that each token is represented by a numerical index in `train_iter`. Directly feeding these indices into the neural network would make learning difficult. Therefore, each token is typically represented as a feature vector that carries more information. The simplest method is to use a one-hot encoding representation\n",
        "\n",
        "In short, we map each index to a unique unit vector: suppose the number of unique tokens in the vocabulary is $N$  (len(vocab)) and their indices range from 0 to  $Nâˆ’1$ . For a token with index $i$ , we create a vector of length  $N$ with all elements set to 0, except for the element at position $i$  which is set to 1. This vector is the one-hot vector of the original token. One-hot vectors for indices 0 and 2 are illustrated below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NoHAL-D6elkq",
        "origin_pos": 9,
        "tab": [
          "tensorflow"
        ]
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 1,  ..., 0, 0, 0]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "F.one_hot(torch.tensor([0, 2]), len(vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38_RJULuelks",
        "origin_pos": 10
      },
      "source": [
        "The minibatch size we sample each time is (batch size, time steps). The `one_hot` function transforms such a minibatch into a 3-dimensional tensor with the size of the last dimension equal to the vocabulary size. We often transpose the input to produce an output with the shape (time steps, batch size, vocabulary size), which is more suitable for feeding into a sequence model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CTNn5SfGelkt",
        "origin_pos": 13,
        "tab": [
          "tensorflow"
        ]
      },
      "outputs": [],
      "source": [
        "def one_hot(X, vocab_size):\n",
        "    # Output shape: (num_steps, batch_size, vocab_size)\n",
        "    return F.one_hot(X.T, vocab_size).type(torch.float32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY2uc7znelku",
        "origin_pos": 18
      },
      "source": [
        "## RNN Model (2 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p align=\"center\">\n",
        "<img src=\"./data/architecture_RNN.png\"/>\n",
        "\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we initialize the hidden state using the `init_rnn_state` function. This function returns a tuple containing an ndarray filled with zeros and with a shape of (batch size, number of hidden units). Returning a tuple makes it easier to handle situations where the hidden state contains multiple variables (e.g., when we need to initialize multiple layers combined in an RNN).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def init_rnn_state(batch_size, num_hiddens, device):\n",
        "    # Init H_0\n",
        "    return (torch.zeros((batch_size, num_hiddens), device=device),)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHeTBEdGelkt",
        "origin_pos": 14
      },
      "source": [
        "### Initializing the Model Parameters (1 point)\n",
        "Next, we initialize the parameters for the RNN model. The number of hidden units `num_hiddens` is a tunable parameter. When training the language model in this case, both the inputs and outputs rely on the same vocabulary, so they will have the same dimension (which is equal to the vocabulary size).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, you are required to initialize the parameters for the RNN model (and don't forget to normalize them) based on the knowledge learned from the lecture and the architecture diagram provided above. \n",
        "\n",
        "Replace `NotImplemented` with your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FFRFI271elkt",
        "origin_pos": 17,
        "tab": [
          "tensorflow"
        ]
      },
      "outputs": [],
      "source": [
        "def get_params(vocab_size, num_hiddens, device):\n",
        "    num_inputs = num_outputs = vocab_size\n",
        "\n",
        "    def normal(shape):\n",
        "        return torch.randn(size=shape, device=device) * 0.01\n",
        "\n",
        "    # Hidden layer parameters\n",
        "    W_xh = NotImplemented\n",
        "    W_hh = NotImplemented\n",
        "    b_h = NotImplemented\n",
        "    # Output layer parameters\n",
        "    W_hy = NotImplemented\n",
        "    b_y = NotImplemented\n",
        "    # Attach gradients\n",
        "    params = [W_xh, W_hh, b_h, W_hy, b_y]\n",
        "    for param in params:\n",
        "        param.requires_grad_(True)\n",
        "    return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IaQ4Wrxelku",
        "origin_pos": 22
      },
      "source": [
        "### Forward function (1 Point)\n",
        "The following `rnn` function defines how to compute the hidden state and output at each time step. The activation function used here is tanh. The average value of the tanh function is 0 when the elements are evenly distributed on the real number line.\n",
        "\n",
        "Replace `NotImplemented` with your code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2ouOOKa8elkv",
        "origin_pos": 25,
        "tab": [
          "tensorflow"
        ]
      },
      "outputs": [],
      "source": [
        "def rnn(inputs, state, params):\n",
        "    # Here `inputs` shape: (`num_steps`, `batch_size`, `vocab_size`)\n",
        "    W_xh, W_hh, b_h, W_hy, b_y = params\n",
        "    H, = state\n",
        "    outputs = []\n",
        "    # Shape of `X`: (`batch_size`, `vocab_size`)\n",
        "    for X in inputs:\n",
        "        # Compute the hidden state H and output Y at each time step\n",
        "        H = NotImplemented\n",
        "        Y = NotImplemented\n",
        "        outputs.append(Y)\n",
        "    return torch.cat(outputs, dim=0), (H,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmT72mSXelkv",
        "origin_pos": 26
      },
      "source": [
        "After defining all the functions, we create a class to encapsulate these functions and store the parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cH0ZHLRjelkw",
        "origin_pos": 29,
        "tab": [
          "tensorflow"
        ]
      },
      "outputs": [],
      "source": [
        "class RNNModelScratch:\n",
        "    \"\"\"A RNN Model implemented from scratch.\"\"\"\n",
        "    def __init__(self, vocab_size, num_hiddens, device, get_params,\n",
        "                 init_state, forward_fn):\n",
        "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
        "        self.params = get_params(vocab_size, num_hiddens, device)\n",
        "        self.init_state, self.forward_fn = init_state, forward_fn\n",
        "\n",
        "    def parameters(self):\n",
        "        for param in self.params:\n",
        "            yield param\n",
        "\n",
        "    def __call__(self, X, state):\n",
        "        X = one_hot(X, vocab_size=self.vocab_size)\n",
        "        return self.forward_fn(X, state, self.params)\n",
        "\n",
        "    def begin_state(self, batch_size, device):\n",
        "        return self.init_state(batch_size, self.num_hiddens, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output size is (number of time steps Ã— batch size, vocabulary size), while the hidden state size remains unchanged at (batch size, number of hidden units).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-urVbhBAelkx",
        "origin_pos": 34
      },
      "source": [
        "\n",
        "### Prediction\n",
        "\n",
        "First, we explain the prediction function, which is commonly used for testing during training. This function predicts the next `num_predicts` tokens based on the `input_sentence` (a string containing a few tokens). For the first tokens in the sequence, we only update the hidden state and then start generating new tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(model, input_sentence, device, num_preds=[5, 10]):\n",
        "    outputs = []\n",
        "    for num_pred in num_preds:\n",
        "        input_words = input_sentence.split()\n",
        "        output_words = input_words.copy()\n",
        "        state = model.begin_state(batch_size=1, device=device)\n",
        "        for input_word in input_words[:-1]:\n",
        "            word_index = torch.tensor(vocab[input_word], device=device).reshape((1, 1))\n",
        "            _, state = model(word_index, state)\n",
        "\n",
        "        word = input_words[-1]\n",
        "        for _ in range(num_pred):\n",
        "            word_index = torch.tensor(vocab[word], device=device).reshape((1, 1))\n",
        "            y, state = model(word_index, state)\n",
        "            output_word = vocab.to_tokens(int(y.argmax(dim=1).reshape(1)))\n",
        "            output_words.append(output_word)\n",
        "            word = output_word\n",
        "        outputs.append(output_words)\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uVOi5lHelky",
        "origin_pos": 41
      },
      "source": [
        "## Gradient Clipping (0.5 points)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For a sequence of length $T $, during a single backpropagation pass, the gradient is computed over $ T $ time steps, resulting in a sequence of matrix multiplications with a complexity of $O(T) $. When $ T $ is large, this can lead to numerical instability, manifesting as exploding or vanishing gradients. Therefore, RNN models often require some support to stabilize training.\n",
        "\n",
        "Recall that in solving optimization problems, we update weights $ w $ in the direction opposite to the gradient $g_t $ for a minibatch, using the formula $ w - \\eta \\cdot g_t $. Suppose the objective function is Lipschitz continuous with constant $ L$, meaning:\n",
        "\n",
        "$$\n",
        "|l(\\mathbf{w}) - l(\\mathbf{w}')| \\leq L \\|\\mathbf{w} - \\mathbf{w}'\\|.\n",
        "$$\n",
        "\n",
        "In this case, the change in the weight vector during an update of $ \\eta \\cdot g_t $ will not exceed $ L \\eta \\|g_t\\| $. This has both advantages and disadvantages. The disadvantage is that the speed of optimization is limited, while the advantage is that the degree of deviation when optimizing in the wrong direction is also constrained.\n",
        "\n",
        "Sometimes the gradient can become quite large, causing the optimization algorithm to fail to converge. This issue can be addressed by reducing the learning rate $ \\eta $ or by employing certain techniques related to higher-order derivatives. However, if the gradient rarely becomes large, such approaches do not guarantee complete convergence. Another method is gradient clipping, where the gradient is projected onto a sphere of radius $ \\theta $ using the formula:\n",
        "\n",
        "$$\n",
        "\\mathbf{g} \\leftarrow \\min\\left(1, \\frac{\\theta}{\\|\\mathbf{g}\\|}\\right) \\mathbf{g}.\n",
        "$$\n",
        "\n",
        "This ensures that the norm of the gradient does not exceed $ \\theta $, and the clipped gradient maintains the same direction as the original gradient $ g $. Gradient clipping has the additional benefit of limiting the influence of any single minibatch (or any single sample) on the weights, making the model more stable. While it does not completely resolve the issue, it is a simple technique to mitigate the problem of exploding gradients.\n",
        "\n",
        "Below, we define the gradient clipping function, which can be used for both the RNNModelScratch implemented from scratch and models built with Gluon. Note that the gradient norm is computed across all parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Implement the following `grad_clipping` function below.\n",
        "\n",
        "Replace `NotImplemented` with your code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "J4XpbekYelky",
        "origin_pos": 44,
        "tab": [
          "tensorflow"
        ]
      },
      "outputs": [],
      "source": [
        "\n",
        "def grad_clipping(net, theta):\n",
        "    \"\"\"Clip the gradient.\"\"\"\n",
        "    if isinstance(net, nn.Module):\n",
        "        params = [p for p in net.parameters() if p.requires_grad]\n",
        "    else:\n",
        "        params = net.params\n",
        "    NotImplemented"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72hVSkqqelkz",
        "origin_pos": 45
      },
      "source": [
        "## Training (2.5 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this final section, you are required to design a train function to train the model above with the following requirements:\n",
        "\n",
        "- This train function must provide different sampling methods (ways to sample the next tokens) for sequential data (random sampling and sequential sampling), which result in differences in how the hidden states are initialized.\n",
        "- Clip the gradients before updating the model parameters. This ensures that the model does not diverge, even if the gradients explode at some point during training, and it effectively reduces the magnitude of the update step automatically.\n",
        "\n",
        "When performing sequential sampling, we only initialize the hidden states at the beginning of each epoch. Since the  $i^{th}$  sample in the next minibatch is adjacent to the  $i^{th}$  sample in the current minibatch, we can directly use the current hidden state for the next minibatch, while detaching the gradients to compute them separately for each minibatch. In contrast, when performing random sampling, we need to reinitialize the hidden states for each iteration of minibatch because each sample is drawn from a random position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data\n",
        "num_steps = 10\n",
        "batch_size = 32\n",
        "corpus, vocab = load_corpus_time_machine()\n",
        "train_iter = SeqLoader(corpus, vocab, num_steps, batch_size)\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "num_hiddens = 512\n",
        "model = RNNModelScratch(len(vocab), num_hiddens, device, get_params, init_rnn_state, rnn)\n",
        "\n",
        "# Optimization\n",
        "num_epochs = 500\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model.parameters(), lr=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### (2.5 points) Write your `train` function that returns the trained model and the loss during training\n",
        "\n",
        "You need to implement both the random sampling method and the sequential sampling method, and plot the loss function during the training process for both methods to earn points for this section.\n",
        "- (2 points) Train function and training process \n",
        "    -  (1 point) Random sampling method\n",
        "    -  (1 point) Sequential sampling method\n",
        "- (0.5 points) The plot of the loss function during the training process "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(model, train_iter, device, criterion, optimizer, num_epochs, random_sampling=True):\n",
        "    # Set random_sampling = True to perform random sampling method \n",
        "    # Set random_sampling = False then perform sequential sampling method\n",
        "    NotImplemented\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "dh8YTUyNelkq"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "saar",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
